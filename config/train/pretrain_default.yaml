batch_size: &batch_size 128
num_epochs: 400

dataloader:
  train:
    TinyImagenetFastDataLoader: # TinyImagenetDataLoader
      batch_size: *batch_size
      train: true
#      num_workers: 2
#      extra_transforms: random_crop
  test:
    TinyImagenetFastDataLoader: # TinyImagenetDataLoader
      batch_size: *batch_size
      train: false
# If you also want train/val split:
#dataloader:
#  train:
#    TinyImagenetDataLoader:
#      batch_size: *batch_size
#      train: true
#      val_size: 0.2
#      use_val: false
#      extra_transforms: random_crop
#  test:
#    TinyImagenetDataLoader:
#      batch_size: *batch_size
#      train: false
#  val:
#    TinyImagenetDataLoader:
#      batch_size: *batch_size
#      train: true
#      val_size: 0.2
#      use_val: true
init_fn:
  xavier_uniform_:
    gain: !eval sqrt(2)  # ReLU gain: sqrt(2)
loss:
  PatchWise:
    criterion:
      MSELoss:
optim:
  AdamW:
    betas: !!python/tuple
      - 0.9
      - 0.95
    lr: !eval 5e-4 * batch_size / 128
    weight_decay: 0.15
scheduler:
  SequentialLR:
    schedulers:
      - LinearLR:
          total_iters: !eval num_epochs // 20
      - CosineAnnealingLR:
          T_max: !eval num_epochs - num_epochs // 20
    milestones:
      - !eval num_epochs // 20